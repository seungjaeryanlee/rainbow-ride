{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a Deep Q-Learning Network (DQN) agent that uses a Neural Network (NN) to estimate the action value (Q value) of a state. Deep Q-Learning Network was invented by Google DeepMind in 2013, and it became famous for achieving superhuman performance on 29 out of 49 games on *Atari 2600* games."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are expected to have basic familiarity with Machine Learning, Deep Learning, and Reinforcement Learning, especially on following topics:\n",
    "\n",
    " * Neural Network\n",
    " * Q-Learning\n",
    " * (Optional) Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are also expected to have basic familarity with PyTorch, especially on following modules:\n",
    "\n",
    " * `torch.nn`\n",
    " * `torch.optim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [Playing Atari with Deep Reinforcement Learning (2013)](https://arxiv.org/pdf/1312.5602.pdf)\n",
    " * [Human-level control through deep reinforcement learning (2015)](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment: CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get familiar with the environment we will use to train and validate our DQN agent on. We will use the **CartPole** environment from [OpenAI Gym](https://gym.openai.com). In the CartPole environment, the agent has two components: a cart and a pole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent's goal is to maintain balance so that the pole stays upright. To maintain balance, the agent must move the cart left or right accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Cartpole](cartpole.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State Space Dimensions**: 4\n",
    " * $x$: Location of the cart\n",
    " * $x'$: Speed of the cart\n",
    " * $\\theta$: Angle of the pole\n",
    " * $\\theta'$: Angular Speed of the pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action Space Size**: 2\n",
    " * Left\n",
    " * Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reward**: +1 for all state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand the environment a bit better, let's render the environment with a random-action agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create an environment from OpenAI Gym, we use `gym.make()` command with appropriate environment name. To initialize the environment, we use `env.reset()` command. `env.reset()` returns the observation of the first state $s_0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[ 0.03656076  0.03471627 -0.00079671  0.04015512]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All OpenAI Gym environment has an `action_space` attribute with a `sample()` method. `env.action_space.sample()` returns a randomly selected action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform an action, we use `env.step(action)`. The environment then returns `(obs, rew, done, info)`, where\n",
    " * `obs` is the observation of the state after the action\n",
    " * `rew` is the reward after the action\n",
    " * `done` is a boolean indicating if the episode has terminated\n",
    " * `info` is a dictionary containing other useful information about the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:  [ 3.72550881e-02 -1.60394250e-01  6.39132317e-06  3.32586567e-01]\n",
      "Reward:  1.0\n",
      "Done:  False\n",
      "Info:  {}\n"
     ]
    }
   ],
   "source": [
    "obs, rew, done, info = env.step(action)\n",
    "print('Observation: ', obs)\n",
    "print('Reward: ', rew)\n",
    "print('Done: ', done)\n",
    "print('Info: ', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To render the environment, we use `env.render()`, and to close the rendered window, we use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us implement a neural network with PyTorch. We will use a simple neural network with few layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a `NaiveDQN` class that inherits `torch.nn.Module`, and define the layers in `NaiveDQN.__init__()` and define the forward propagation step in `NaiveDQN.forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveDQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dims, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dims)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huber Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Environment: Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DQN paper by Google DeepMind, they tested their agents with games from *Atari 2600*. We can also try training and evaluating the DQN agent's performance on *Atari 2600* games. Specifically, we will use *Pong* as our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](pong.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State Space Dimensions**: (210, 160, 3)\n",
    " * Height of 210 pixels\n",
    " * Width of 160 pixels\n",
    " * Pixels represented by RGB values: 3 numbers ranging from 0~255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Action Space Size**: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')\n",
    "obs = env.reset()\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional Agent: CNN DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike CartPole, the observation we receive from the environment is an image of shape (210, 160, 3). Thus, we should use a Convolutional Neural Network (CNN) in the DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNDQN(nn.Module):\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [PyTorch: Reinforcement Learning (DQN) Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    " * [RL Adventure by higgsfield](https://github.com/higgsfield/RL-Adventure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rainbow]",
   "language": "python",
   "name": "conda-env-rainbow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
