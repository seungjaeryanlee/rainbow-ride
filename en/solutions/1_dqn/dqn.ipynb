{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning Network (DQN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a Deep Q-Learning Network (DQN) agent that uses a Neural Network (NN) to estimate the action value (Q value) of a state. Deep Q-Learning Network was invented by Google DeepMind in 2013, and it became famous for achieving superhuman performance on 29 out of 49 games on *Atari 2600* games.\n",
    "\n",
    "The links to DQN papers are at the end in the **References** section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You are expected to have basic familiarity with Machine Learning, Deep Learning, and Reinforcement Learning, especially on following topics:\n",
    "\n",
    " * Neural Network\n",
    " * Q-Learning\n",
    " * (Optional) Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You are also expected to have basic familarity with PyTorch, especially on following modules:\n",
    "\n",
    " * `torch.nn`\n",
    " * `torch.optim`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Environment: CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### OpenAI Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, let's get familiar with the environment we will use to train and validate our DQN agent on. We will use the **CartPole** environment from [OpenAI Gym](https://gym.openai.com). In the CartPole environment, the agent has two components: a cart and a pole."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "At every timestep, the agent receives +1 reward unless the episode is terminated. The episode terminates if\n",
    "\n",
    " 1. the cart is too far left or right\n",
    " 2. the pole  is too far off from its upright position\n",
    " 3. 200 timesteps passed.\n",
    "\n",
    "Thus, the agent's goal is to maintain balance so that the pole stays upright. To maintain balance, the agent must move the cart left or right accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![Cartpole](cartpole.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**State Space Dimensions**: 4\n",
    " * $x$: Location of the cart\n",
    " * $x'$: Speed of the cart\n",
    " * $\\theta$: Angle of the pole\n",
    " * $\\theta'$: Angular Speed of the pole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Action Space Size**: 2\n",
    " * Left\n",
    " * Right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Reward**: +1 for all state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To understand the environment a bit better, let's render the environment with a random-action agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To create an environment from OpenAI Gym, we use `gym.make()` command with appropriate environment name. To initialize the environment, we use `env.reset()` command. `env.reset()` returns the observation of the initial state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "[ 0.00513888 -0.04807409 -0.04525914 -0.01794885]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "obs = env.reset()\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Observation Space and Action Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "All OpenAI Gym environment has an `observation_space` attribute and an `action_space` attribute. They help you understand more about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space:  Box(4,)\n",
      "Action Space:  Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print('Observation Space: ', env.observation_space)\n",
    "print('Action Space: ', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`Box()` and `Discrete()` are spaces defined in OpenAI Gym. `Box(4,)` means that the observation space has dimension of 4, whereas `Discrete(2)` means that the action space has two discrete values. You can retrieve the space dimensions or size with their attributes. The `Box()` space has a `.shape` attribute, and the `Discrete()` space has a `.n` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation Space Shape:  (4,)\n",
      "Action Space Size:  2\n"
     ]
    }
   ],
   "source": [
    "print('Observation Space Shape: ', env.observation_space.shape)\n",
    "print('Action Space Size: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `Discrete()` space also has a `sample()` method. `env.action_space.sample()` returns a randomly selected action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "print(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Perform an Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To perform an action, we use `env.step(action)`. The environment then returns `(obs, rew, done, info)`, where\n",
    " * `obs` is the observation of the state after the action\n",
    " * `rew` is the reward after the action\n",
    " * `done` is a boolean indicating if the episode has terminated\n",
    " * `info` is a dictionary containing other useful information about the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation:  [ 0.0041774   0.14766672 -0.04561811 -0.32456105]\n",
      "Reward:  1.0\n",
      "Done:  False\n",
      "Info:  {}\n"
     ]
    }
   ],
   "source": [
    "action = env.action_space.sample()\n",
    "obs, rew, done, info = env.step(action)\n",
    "print('Observation: ', obs)\n",
    "print('Reward: ', rew)\n",
    "print('Done: ', done)\n",
    "print('Info: ', info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To render the environment, we use `env.render()`, and to close the rendered window, we use `env.close()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    obs, rew, done, info = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement a naive version of a Deep Q-Learning Network. The idea is simple: transform a tabular Q-Learning algorithm into a approximate Q-Learning algorithm by simply using a neural network to approximate the action value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a refresher, here is the tabular Q-Learning algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q(s, a) := Q(s, a) + \\alpha \\big(r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\big) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `NaiveDQNAgent` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In each subsection, we will implement parts of the `NaiveDQNAgent` class, defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class NaiveDQNAgent:\n",
    "    \"\"\"\n",
    "    A reinforcement learning agent that uses DQN naively to estimate action\n",
    "    values.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, dqn, discount_factor=0.99):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        env\n",
    "            An OpenAI Gym environment.\n",
    "        dqn\n",
    "            An instance of Deep Q Network written in PyTorch, inheriting\n",
    "            nn.Module.\n",
    "        discount_factor : float\n",
    "            A float between 0 and 1 denoting the discount factor. A discount\n",
    "            factor of 0 indicates a myopic agent, and a discount factor of 1\n",
    "            indicates an agent that accounts all future rewards without\n",
    "            discount.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.dqn = dqn\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Return an action with respect to the epsilon-greedy policy.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        state : list of float\n",
    "            The current state given by the environment.\n",
    "        epsilon : float\n",
    "            The possibility of selecting a random action.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        action : int\n",
    "            The action chosen by the agent.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, n_steps=1000, show=True):\n",
    "        \"\"\"\n",
    "        Train the agent for specified number of steps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_steps : int\n",
    "            Number of timesteps to train the agent for.\n",
    "        show : bool\n",
    "            If True, plots the episode rewards and losses on Jupyter Notebook.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def play(self, show=True):\n",
    "        \"\"\"\n",
    "        Play an episode and return the total reward for the episode.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        show : bool\n",
    "            If true, render the environment.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can add a function `MyFunction` to a class with the command `NaiveDQNAgent.MyFunction = MyFunction`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "In general, it is not a good practice to write class functions outside the class. Consider this an exception for clarity throughout this Jupyter Notebook :)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's implement a neural network with PyTorch. We will use a simple neural network with few layers. The input will be the observation $s=(x, x', \\theta, \\theta')$, and the output will be the action values $Q(s, \\text{left})$ and $Q(s, \\text{right})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We create a `DQN` class that inherits `torch.nn.Module`. Let's define the layers in `DQN.__init__()` and define the forward propagation step in `DQN.forward()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dims, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dims)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's make sure that the `DQN` is working properly by giving a zero vector as input. We will use the observation given by the environment as our state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1404, -0.0062])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "dqn.forward(torch.zeros(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Choose Action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We first define `NaiveDQNAgent.act()` that chooses an action using the DQN with an epsilon-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def act(self, state, epsilon):\n",
    "    \"\"\"\n",
    "    Return an action with respect to the epsilon-greedy policy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    state : list of float\n",
    "        The current state given by the environment.\n",
    "    epsilon : float\n",
    "        The possibility of selecting a random action.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    action : int\n",
    "        The action chosen by the agent.\n",
    "    \"\"\"\n",
    "    if random.random() > epsilon:\n",
    "        state = torch.FloatTensor(state)\n",
    "        q_values = self.dqn.forward(state)\n",
    "        action = int(q_values.argmax())\n",
    "    else:\n",
    "        action = self.env.action_space.sample()\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NaiveDQNAgent.act = act"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's test the `act()` function. We use the the initial observation from `env.reset()` as our state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "dqn = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "agent = NaiveDQNAgent(env, dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greedy action:  1\n",
      "Random action:  0\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()\n",
    "greedy_action = agent.act(state, epsilon=0)\n",
    "random_action = agent.act(state, epsilon=1)\n",
    "print('Greedy action: ', greedy_action)\n",
    "print('Random action: ', random_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Awesome! We now have an agent that can use DQN to choose an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In Supervised Learning, every data in the training set had a **label** $y$ that represented the correct answer. Therefore, we could calculate the loss by comparing the prediction $\\hat{y}$ and $y$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\text{MSE} = (\\hat{y} - y)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In Reinforcement Learning, the agent does not know the correct action value $Q(s, a)$. Thus, in Q-Learning, we use the bootstrapped **target** instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "$$ \\text{MSE} = (r + \\gamma\\max_{a'}(Q(s', a') - Q(s, a))^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _compute_loss(self, state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    Compute the MSE loss given a transition (s, a, r, s').\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    state : list of float\n",
    "        The current state s given by the environment.\n",
    "    action : int\n",
    "        Action a chosen by the agent.\n",
    "    reward : float\n",
    "        Reward given by the environment for given state s and action a.\n",
    "    next_state : list of float\n",
    "        The resulting state s' after taking action a on state s.\n",
    "    done: bool\n",
    "        True if s' is a terminal state. False otherwise.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : torch.tensor\n",
    "        The MSE loss of the DQN.\n",
    "    \"\"\"\n",
    "    state = torch.FloatTensor(state)\n",
    "    q_values = self.dqn(state)\n",
    "    q_value = q_values[action]\n",
    "\n",
    "    next_state = torch.FloatTensor(next_state)\n",
    "    next_q_values = self.dqn(next_state)\n",
    "    next_q_value = next_q_values.max()\n",
    "\n",
    "    if done:\n",
    "        target = reward\n",
    "    else:\n",
    "        target = reward + self.discount_factor * next_q_value\n",
    "\n",
    "    loss = (q_value - target).pow(2).mean()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NaiveDQNAgent._compute_loss = _compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "dqn = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "agent = NaiveDQNAgent(env, dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss:  tensor(1.0377)\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = agent.act(state, epsilon=1)\n",
    "next_state, reward, done, _ = env.step(action)\n",
    "loss = agent._compute_loss(state, action, reward, next_state, done)\n",
    "print('MSE Loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "`PyTorch` has multiple optimizers in `torch.optim` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We will use the Adam optimizer `torch.optim.Adam()`. To create an optimizer, we call `torch.optim.Adam()` with the parameters of the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(dqn.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Calling `.backward()` on a tensor calculates the gradients. `optimizer.step()` uses the gradients to update the parameters of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Train Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training the model is simple: for every step, we compute the loss with `compute_loss` and perform Stochastic Gradient Descent (SGD) with `optimizer`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To understand the agent's performance during training, we will use `matplotlib` to plot the episode reward and MSE loss while the agent is being trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def _plot(self, step, rewards, losses):\n",
    "    \"\"\"\n",
    "    Plot the total episode rewards and losses per timestep.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rewards : list of float\n",
    "        List of total rewards for each episode.\n",
    "    losses : list of float\n",
    "        List of losses for each timestep.\n",
    "    \"\"\"\n",
    "    clear_output(True)\n",
    "    plt.figure(figsize=(20, 5))\n",
    "    plt.subplot(131)\n",
    "    plt.title('Total Episode Reward')\n",
    "    plt.plot(rewards)\n",
    "    plt.subplot(132)\n",
    "    plt.title('MSE Loss')\n",
    "    plt.plot(losses)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NaiveDQNAgent._plot = _plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def train(self, n_steps=1000, show=False):\n",
    "    \"\"\"\n",
    "    Train the agent for specified number of steps.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_steps : int\n",
    "        Number of timesteps to train the agent for.\n",
    "    show : bool\n",
    "        If True, plots the episode rewards and losses on Jupyter Notebook.\n",
    "    \"\"\"\n",
    "    epsilon = 0.01\n",
    "    state = self.env.reset()\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    episode_reward = 0\n",
    "    losses = []\n",
    "    optimizer = optim.Adam(self.dqn.parameters())\n",
    "    for step_i in range(n_steps):\n",
    "        action = self.act(state, epsilon)\n",
    "        next_state, reward, done, _  = self.env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = self._compute_loss(state, action, reward, next_state, done)\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            state = self.env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "        if step_i % 200 == 0 and show:\n",
    "            self._plot(step_i, episode_rewards, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NaiveDQNAgent.train = train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "dqn = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "agent = NaiveDQNAgent(env, dqn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.train(1000, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Play"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see how good the trained agent is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def play(self, show=True):\n",
    "    \"\"\"\n",
    "    Play an episode and return the total reward for the episode.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    show : bool\n",
    "        If true, render the environment.\n",
    "    \"\"\"\n",
    "    total_reward = 0\n",
    "    state = self.env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = self.act(state, epsilon=0)\n",
    "        next_state, reward, done, _  = self.env.step(action)\n",
    "        if show:\n",
    "            self.env.render()\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "    self.env.close()\n",
    "\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NaiveDQNAgent.play = play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Total reward:  10.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "dqn = DQN(env.observation_space.shape[0], env.action_space.n)\n",
    "agent = NaiveDQNAgent(env, dqn)\n",
    "\n",
    "print('Total reward: ', agent.play())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The maximum total reward for each episode is $200.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the Naive DQN worked decently for `CartPole` environment, it performs poorly in more complex environments like *Atari 2600* due to its instability. To alleviate these problems, the following improvements are made to the naive DQN:\n",
    "\n",
    " * Experience Replay (Mnih, 2013)\n",
    " * Target Network (Mnih, 2015)\n",
    " * Huber Loss (Mnih, 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### `DQNAgent` class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Adding the three modifications (Experience Replay, Target Network, Huber Loss) makes our DQN complete: we have implemented all the algorithms specified in the papers *Playing Atari with Deep Reinforcement Learning* (2013) and *Human-level control through deep reinforcement learning (2015)*.\n",
    "\n",
    "Let's implement the `DQNAgent` class that utilizes these modifications and test it on the `CartPole` environment to compare it with our `NaiveDQNAgent` implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    A reinforcement learning agent that uses DQN naively to estimate action\n",
    "    values.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, DQN,\n",
    "                 replay_buffer,\n",
    "                 discount_factor=0.99,\n",
    "                 min_buffer=100,\n",
    "                 target_update_rate=64):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        env\n",
    "            An OpenAI Gym environment.\n",
    "        DQN\n",
    "            An Deep Q Network class written in PyTorch, inheriting\n",
    "            nn.Module.\n",
    "        replay_buffer\n",
    "            The replay buffer for Experience Replay.\n",
    "        discount_factor : float\n",
    "            A float between 0 and 1 denoting the discount factor. A discount\n",
    "            factor of 0 indicates a myopic agent, and a discount factor of 1\n",
    "            indicates an agent that accounts all future rewards without\n",
    "            discount.\n",
    "        min_buffer : int\n",
    "            Minimum replay buffer size before training the DQN.\n",
    "        target_update_rate : int\n",
    "            Frequency that the target network updates its parameters.\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        input_dims = env.observation_space.shape[0]\n",
    "        output_dims = env.action_space.n\n",
    "        self.dqn = DQN(input_dims, output_dims)\n",
    "        self.target_dqn = DQN(input_dims, output_dims)\n",
    "        self.replay_buffer = replay_buffer\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.discount_factor = discount_factor\n",
    "        self.min_buffer = min_buffer\n",
    "        self.target_update_rate = target_update_rate\n",
    "\n",
    "    def train(self, n_steps=1000, show=True):\n",
    "        \"\"\"\n",
    "        Train the agent for specified number of steps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_steps : int\n",
    "            Number of timesteps to train the agent for.\n",
    "        show : bool\n",
    "            If True, plots the episode rewards and losses on Jupyter Notebook.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Q-Learning, the agent only looks at each experience once: after taking an action, it uses the experience $(s, a, r, s')$ once and forgets it immediately after the update. **Experience Replay** considers a different approach: the agent remembers the last $D$ experiences by saving them into a \"replay memory\". Also, instead of updating the parameters with the new experience, the agent updates its parameters by sampling a batch from the replay memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach has several advantages:\n",
    "\n",
    " 1. Every experience is used multiple times, so it is more efficient in terms of data.\n",
    " 2. Experiences are strongly correlated in the Naive DQN. Randomly sampling from replay memory breaks these correlations and thus reduces the variance of the updates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a `ReplayBuffer` class. The `ReplayBuffer` needs two functions:\n",
    "\n",
    " * `ReplayBuffer.add_sample()` that appends the new experience\n",
    " * `ReplayBuffer.sample()` that uniformly samples a batch of experiences from the buffer\n",
    "\n",
    "We will use Python's `deque` (pronounced \"deck\", not \"dequeue\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add_sample(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Add given experience as sample.\n",
    "        \"\"\"\n",
    "        deque.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Return a batch of samples.\n",
    "        \"\"\"\n",
    "        return random.sample(deque, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also change the `train()` function so that the agent updates its parameters with a batched sample of experiences instead of the last experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, n_steps=1000, show=False):\n",
    "    \"\"\"\n",
    "    Train the agent for specified number of steps.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n_steps : int\n",
    "        Number of timesteps to train the agent for.\n",
    "    show : bool\n",
    "        If True, plots the episode rewards and losses on Jupyter Notebook.\n",
    "    \"\"\"\n",
    "    epsilon = 0.01\n",
    "    state = self.env.reset()\n",
    "    done = False\n",
    "    episode_rewards = []\n",
    "    episode_reward = 0\n",
    "    losses = []\n",
    "    optimizer = optim.Adam(self.dqn.parameters())\n",
    "    for step_i in range(n_steps):\n",
    "        action = self.act(state, epsilon)\n",
    "        next_state, reward, done, _  = self.env.step(action)\n",
    "        episode_reward += reward\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = self._compute_loss(state, action, reward, next_state, done)\n",
    "        losses.append(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if done:\n",
    "            state = self.env.reset()\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_reward = 0\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "        if step_i % 200 == 0 and show:\n",
    "            self._plot(step_i, episode_rewards, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Target Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Target Network** $\\hat{Q}$ is a method of using a separate network to generate the target $r + \\gamma \\max_{a'} \\hat{Q}(s', a')$. The target network $\\hat{Q}$ is updated periodically by cloning the parameters of the original network $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Reward Clipping and Huber Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Reward clipping** is a technique of clipping reward given by the environment. The paper suggests clipping reward to a [-1, 1] interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Huber loss** is a simple transformation done to the error term $r + \\gamma \\max_a' \\hat{Q}(s', a') - Q(s, a)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Optional Environment: Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the DQN paper by Google DeepMind, they tested their agents with games from *Atari 2600*. We can also try training and evaluating the DQN agent's performance on *Atari 2600* games. Specifically, we will use *Pong* as our environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](pong.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**State Space Dimensions**: (210, 160, 3)\n",
    " * Height of 210 pixels\n",
    " * Width of 160 pixels\n",
    " * Pixels represented by RGB values: 3 numbers ranging from 0~255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Action Space Size**: 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('PongNoFrameskip-v4')\n",
    "obs = env.reset()\n",
    "print(obs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space.n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Optional Agent: CNN DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Unlike CartPole, the observation we receive from the environment is an image of shape (210, 160, 3). Thus, we should use a Convolutional Neural Network (CNN) in the DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class CNNDQN(nn.Module):\n",
    "    def __init__(self, input_dims, output_dims):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "    def act(self, state, epsilon):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [Playing Atari with Deep Reinforcement Learning (2013)](https://arxiv.org/pdf/1312.5602.pdf)\n",
    " * [Human-level control through deep reinforcement learning (2015)](https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " * [PyTorch: Reinforcement Learning (DQN) Tutorial](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html)\n",
    " * [RL Adventure by higgsfield](https://github.com/higgsfield/RL-Adventure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "493px",
    "left": "1835px",
    "right": "20px",
    "top": "115px",
    "width": "576px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
